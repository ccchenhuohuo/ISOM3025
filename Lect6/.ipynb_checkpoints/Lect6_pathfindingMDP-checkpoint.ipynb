{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "822f3fb2",
   "metadata": {},
   "source": [
    "% ISOM 3025 Lect6\n",
    "% Yi Ding\n",
    "% 26 November 2022\n",
    "\n",
    "# Lecture 6: Markov Decision Application: Path finding \n",
    "\n",
    "## Outlines\n",
    "\n",
    "In this lecture:\n",
    "\n",
    "* Use Markov decision process to model path finding\n",
    "* Introduce Bellman equation and learning strategy to solve the problem\n",
    "\n",
    "\n",
    "\n",
    "## Path finidng problem\n",
    "\n",
    "Markov Decision Processes can be used to find the best path through a maze. Let us consider this simple maze.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Maze](maze.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "A person wants to reach point \"X\". The boundaries and blocks in the maze are in black. \n",
    "\n",
    "\n",
    "\n",
    "At a particular state, the person will need to move one step horitentally or vetically and she needs to determine which direction to go. \n",
    "\n",
    "We would like to find the optimal strategy (shortest path) from $s_0$ to $s^*$. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aead508b",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### State space\n",
    "\n",
    "We first quantify the location of the person in the maze by coordiates $(x,y)$, where $x\\in 1, 2, .., 6$, and $y\\in (1,2, ... 6)$, say that the square on the bottom right corner has coordinats $(1,1)$. \n",
    "\n",
    "Therefore, totally there are at most 36 locations. $(1,1),(1,2), ..., (1, 6), ..., (6,6)$, some of the states (block area) are not feasible.\n",
    "\n",
    "The set of the blocks is $$Blocks_s=\\{(1,1),(1,2),(2,5),(3,1),(3,3),(4,3),(4,5),(5,5),(5,6),(6,2),(6,3)\\}.$$\n",
    "\n",
    "The set of feasible states is therefore $$S=\\{(1,3),(1,4),(1,5),(1,6),(2,1),(2,2),(2,3),(2,4),(2,6), (3,2), (3,4), (3,5), (3,6), (4,1),(4,2), (4,4), (4,6), (5,1), (5,2), (5,3), (5,4), (6,1), (6,2), (6,4), (6,5)\\}.$$ \n",
    "\n",
    "We see that there are totally 25 states.\n",
    "\n",
    "\n",
    "The points outside the boundaries are characterized as follows\n",
    "$$\n",
    "Boundary_s=\\{(x,y):x<1 \\text{; or }x>6\\text{;or } y<1 \\text{;or } y>6\\}\n",
    "$$\n",
    "\n",
    "The start point is  $s_0(1,6)$, \n",
    "\n",
    "The terminal point is $s^*=(6,1)$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18445f5e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Action space\n",
    "\n",
    "For each states, say $s=(1,6)$, she can either move right for one step, or down one step. In generally, we code the movement \n",
    "$(right, left, up, down)$ by $$A=\\{(1,0),(-1,0), (0,1), (0,-1)\\},$$ respectively. \n",
    "\n",
    "In order to not hitting the block, we will make moving towards the block as a ``bad move\" that is associated with a large negative rewards. Also, the following state transition function avoids the location moves to infeasible space. \n",
    "\n",
    "\n",
    "### The state transition\n",
    "\n",
    "We suppose that the agent can move as she wish within the boundaries and when she does not  hit the rock, otherwise, she will remain at her current position. \n",
    "$$\n",
    "s_{t+1}=s_t+a_t \\text{ if } s_{t}+(a_t) \\notin Boundary_s \\cup Blocks_s;\n",
    "$$\n",
    "$$s_{t+1}=s_t\\text{ if } s_{t}+(a_t) \\in Boundary_s \\cup Blocks_s.\n",
    "$$\n",
    "\n",
    "\n",
    "### Rewards\n",
    "    \n",
    "1. We make the reward to be invariance for all plausible state, this is reasonable because when playing the maze, we do not be able to know the exact location relationship between the state and the terminal, hence no jugdgement or preference about any specific location. For example, $r(a_t, s_t)=0.1$, for all $s_{t+1}$ not in the blocks, boundaries or terminal point.\n",
    "\n",
    "2. We make the reward to be negative when the person will be about to hit the blocking area or out of boundary after a certain action. \n",
    "That is, for example,\n",
    "$r(a_t,s_t)=-5$, if $s_t+a_t\\in Blocks_s\\cup Boundary_s$ .\n",
    "\n",
    "3. We make the terminal state to be associated with a high positive reward. For example, \n",
    "$r(a_t, s_t)=5$ if $s_{t+1}=s^*$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e94f52",
   "metadata": {},
   "source": [
    "**Value function**\n",
    "\n",
    "We evaluate the overall effectiveness of a certain strategy with the cumulative value of all the actions in the strategy. Specifically, we consider the following value function\n",
    "\n",
    "$$V(s_0,\\pi)=E(\\sum_{t=0}^{\\infty}\\gamma^{t} r(\\pi(s_t),s_t)),$$\n",
    "where $\\pi()$ is a deterministic policy, it is projection of the states space into action space, e.g., $\\pi((1,6))=(1,0)$, $\\gamma$ is a discount factor, e.g., we can set it to be 0.8. \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9627e1",
   "metadata": {},
   "source": [
    "**Example**: Suppose that we adopt a strategy $\\pi(a, s)$ such that make the person move right for any state,  $\\pi(s)=[1,0]$ for all $s$. What is value of such strategy?\n",
    "\n",
    "\n",
    "\n",
    "* at time $0$, from the intial point move one step to the right: $s_1=(2,6)$, reward is 0.1\n",
    "\n",
    "* at time $1$, from $s_1=(2,6)$ to $s_2=c(3,6)$, reward is 0.1.\n",
    "\n",
    "* at time $2$, from $s_2=(3,6)$ to $s_3=(4,6)$, reward is 0.1.\n",
    "\n",
    "* at time $3$, from $s_3=(4,6)$ to $s_4=(4,6)$, because there is a block, reward is -5\n",
    "\n",
    "* at time $t\\geq 4$, if keep doing the same move $[1,0]$, then the value will be\n",
    "$$V(s_0,\\pi)=0.1+0.1\\gamma+0.1\\gamma^2-5\\gamma^3-5\\gamma^4...=0.1+0.1\\gamma+0.1\\gamma^2-5\\gamma^{3}/(1-\\gamma).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86497fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2e04af3",
   "metadata": {},
   "source": [
    "By our above modeling, we translate the maze solving into finding the optimal strategy $\\pi^*$ that achives the highest value\n",
    "\n",
    "$$\\pi^*=argmax_{\\pi} V(s_0,\\pi).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e656f5",
   "metadata": {},
   "source": [
    "Let us first setup the Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3a834b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python realization of the modeling:\n",
    "\n",
    "\n",
    "StatesData = [[1,1],[1,2],[1,3],[1,4],[1,5],[1,6],\n",
    "              [2,1],[2,2],[2,3],[2,4],[2,5],[2,6],\n",
    "              [3,1],[3,2],[3,3],[3,4],[3,5],[3,6],\n",
    "              [4,1],[4,2],[4,3],[4,4],[4,5],[4,6],\n",
    "              [5,1],[5,2],[5,3],[5,4],[5,5],[5,6],\n",
    "              [6,1],[6,2],[6,3],[6,4],[6,5],[6,6]]\n",
    "# StatesData documents the state space. \n",
    "\n",
    "Initial_State=[1,6] # starting point\n",
    "Terminal_State=[6,1] # terminal point\n",
    "\n",
    "Blocks=[[1,1],[1,2],[2,5],[3,1],[3,3],[4,3],[4,5],[5,5],[5,6],[6,2],[6,3]]\n",
    "#print(Blocks)\n",
    "#print(StatesData)\n",
    "\n",
    "Actions=[[1,0],[-1,0],[0,1],[0,-1]] # action space\n",
    "def states_fun(action, state):\n",
    "    state_tplusone=[action[0]+state[0],action[1]+state[1]]\n",
    "    if (state_tplusone[0]<1)|(state_tplusone[0]>6)|(state_tplusone[1]<1)|(state_tplusone[1]>6)|(state_tplusone in Blocks):\n",
    "        state_tplusone=state\n",
    "        \n",
    "    return state_tplusone\n",
    "def rewards_fun(action, state):\n",
    "    state_tplusone=[action[0]+state[0],action[1]+state[1]]\n",
    "    if (state_tplusone[0]<1)|(state_tplusone[0]>6)|(state_tplusone[1]<1)|(state_tplusone[1]>6)|(state_tplusone in Blocks):\n",
    "        return -5\n",
    "    if state_tplusone!= Terminal_State:\n",
    "        return 0.1\n",
    "    if state_tplusone== Terminal_State:\n",
    "        return 5\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "10310606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([3,5] in Blocks)\n",
    "\n",
    "print([1,1] in Blocks)\n",
    "\n",
    "state=[1,2]\n",
    "print((state[0]<1)|(state[0]>6)|(state[1]<1)|(state[1]>6)|(state in Blocks))\n",
    "\n",
    "(state in Blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10695d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n",
      "[1, 3]\n",
      "[2, 3]\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "state=[1,3]\n",
    "action=Actions[0]\n",
    "print(action)\n",
    "print(state)\n",
    "print(states_fun(action, state))\n",
    "print(rewards_fun(action,state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e749197",
   "metadata": {},
   "source": [
    "### Solving the path finding problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c0dc15",
   "metadata": {},
   "source": [
    "**Bellman equation**\n",
    "\n",
    "$$V(s_t,\\pi)=r(s_t,a_t)+\\gamma V(s_{t+1},\\pi), $$where $s_{t+1}$ is the state at time $t+1$ after taking action $a_t$ by the policy $\\pi$. \n",
    "\n",
    "Then we have that, for the optimal policy $\\pi^*$,\n",
    "$$V(s_t,\\pi^*)=max_{a_t}(r(s_t,a_t)+\\gamma V(\\delta(s_t,a_t),\\pi^*), $$where $\\delta(s_t,a_t)=s_{t+1}$ is the state at time $t+1$ after taking the action $a_t$ determined by the policy $\\pi^*$, i.e., $a_t=\\pi^*(s_t)$.\n",
    "\n",
    "In other words, optimal strategy is optimal in any subsequent states. \n",
    "\n",
    "We can use recursive backward induction to find the optimal policy.\n",
    "\n",
    "We have \n",
    "$$\\pi^*(s)=argmax_{a} (r(s,a)+\\gamma V(\\delta(s,a),\\pi^*),$$ where $\\delta(s,a)$ is the next state after taking action.\n",
    "\n",
    "\n",
    "We can then use the above equation to recursively perform policy updating. Say that from strategy $\\pi_k$, we update it to $\\pi_{k+1}$ such that\n",
    "\n",
    "$$\n",
    "\\pi_{k+1}(s)=argmax_{a} r(s,a)+\\gamma V(s'|\\pi_k), \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e29f8",
   "metadata": {},
   "source": [
    "Explicitly solving the Bellman optimality equation provides one route to\n",
    "finding an optimal policy, and thus to solving the reinforcement learning problem. However, this solution is rarely directly useful. It is akin to an exhaustive\n",
    "search, looking ahead at all possibilities, computing their probabilities of occurrence and their desirabilities in terms of expected rewards. This solution\n",
    "relies on at least three assumptions that are rarely true in practice: (1) we\n",
    "accurately know the dynamics of the environment; (2) we have enough computational resources to complete the computation of the solution; and (3) the\n",
    "Markov property. For the kinds of tasks in which we are interested, one is generally not able to implement this solution exactly because various combinations of these assumptions are violated. For example, although the first\n",
    "and third assumptions present no problems for the game of backgammon, the\n",
    "second is a major impediment. Since the game has about 100*100=10000 states, it would\n",
    "take thousands of years on today’s fastest computers to solve the Bellman\n",
    "equation In reinforcement learning, one typically has to settle for approximate solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdccb704",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The problem can be then be solved using dynamic programming (DP). \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db6f1f",
   "metadata": {},
   "source": [
    "### Dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361bed00",
   "metadata": {},
   "source": [
    "Here are some ideas for the Pseudo-codes to implement the dynamic programming algorithm. \n",
    "\n",
    "1. Initialization: \n",
    "\n",
    "a randomly chosen trategy $\\pi$, maps states to actions.\n",
    "\n",
    "2. Recursively update date the strategy until convergence: for $k=0, ..., $\n",
    "\n",
    "    \n",
    "   * 2.1.Evaluation: for all $s\\in S$:\n",
    "     \n",
    "        compute the value $V(s, \\pi)$\n",
    "     \n",
    "   * 2.2. Improvement: for all $s\\in S$:\n",
    "    \n",
    "       * 2.2.1. make 4 candidate strategies, $\\pi_{1}$, ..., $\\pi_{4}$, such that $\\pi_{i}(s)=\\pi_{k}$ for all $s\\neq s_k$, and $\\pi_{i}=a_i$.\n",
    "    \n",
    "       * 2.2.2. compute value of strategies $\\pi_{i}$, that is $V(s,\\pi_{i})=r(s,\\delta(s,\\pi_i(s))+\\gamma V(\\delta(s,\\pi_i(s)),\\pi)$ for $i=1,..., 4$.\n",
    "   \n",
    "       * 2.2.3. the updated strategy $\\pi$ to be the one that has the largest value among  $\\pi_{1}$,  $\\pi_{2}$, $\\pi_{3}$, $\\pi_{4}$. \n",
    "     \n",
    "   \n",
    "  \n",
    "3. Output:  the optimal strategy $\\pi$, the value table $V(s,\\pi)$.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf34012",
   "metadata": {},
   "source": [
    "Each policy is guaranteed to be a strict improvement over the previous one(unless it is already optimal). Because a finite MDP has only a finite number of policies, this process must converge to an optimal policy and optimal value function in a finite number of iterations.\n",
    "This way of finding an optimal policy is called policy iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcbc1bd",
   "metadata": {},
   "source": [
    "Detail implementations of the python programming is left for interesting reader (not required)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40bb184",
   "metadata": {},
   "source": [
    "\n",
    "The DP Approach above requires complete knowledge of the environment. Also, computation complexity can be high for system with a large number of  states. \n",
    "\n",
    "Other approaches: \n",
    "\n",
    "Monte Carlo methods, require only experience—sample sequences of states, actions, and rewards from actual or simulated interaction with an environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb448c4d",
   "metadata": {},
   "source": [
    "    \n",
    "Further topics for interesting reader (not required)\n",
    "\n",
    "Keywords: Reinforcement learning; Q-learning; epsilon-greedy; Monte-Carlo; etc. \n",
    "\n",
    "Further references and resources:\n",
    "\n",
    "Richard S. Sutton and Andrew G. Barto (2014): Reinforcement Learning: An Introduction. Chapter 3-6. \n",
    "\n",
    "\n",
    "Example codes of implementation:\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/174764973\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/maze-rl-d035f9ccdc63\n",
    "\n",
    "https://strikingloo.github.io/reinforcement-learning-beginners\n",
    "\n",
    "https://towardsdatascience.com/hands-on-introduction-to-reinforcement-learning-in-python-da07f7aaca88\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ed149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
